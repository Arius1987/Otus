## Вторая лабораторная работа.

В ней мы развернем underlay сетку и L3 vxlan. После чего прикрутим ipsec - шифрование (раз предлагают в оф.документации, почему бы не попробовать)

Вот схема сети:

![image](https://github.com/user-attachments/assets/01cfdeaf-9f22-4d6f-8a89-3e977ed2408d)



Сначала, как бы это странно не выглядело, мы настроим overlay сеть силами графического интерфейса proxmox, данные настройки запишутся в файл /etc/frr/frr.conf и /etc/network/interfaces.d/sdn, затем пропишем статические маршруты к другому проксмоксу.
Вы наверное возмутитесь тому, что маршруты будут статическими, но в процессе подготовки лабораторной я обратил внимание, что при настроенном ospf сетка рабатает нестабильно

Приступим: в вэб-интерфейсе proxmox (192.168.5.23) зададим в Datacenter-SDN-Options. Жмем Controllers-Add-Evpn:

![image](https://github.com/user-attachments/assets/c298d940-2545-4e81-9527-c5bd2fa85809)


 - физический интерфейс проксмокса слева смотрящего на циску, с которым поднимем evpn пиринг, почему не к lo интерфейсу, спросите вы, ответ будет дальше
 - 65000 - Наша AS (iBGP)


Далее в Datacenter-SDN-Zones

![image](https://github.com/user-attachments/assets/2e2380b1-2572-4fac-8bbc-423f5bffeee3)


Вкратце о настройках:

id - название

Controller - выбор настроенного ранее контроллера

VRF VXLAN TAG - номер vni

VNet MAC Address - согласно документации: "адрес любой рассылки, который назначается всем виртуальным сетям в этой зоне. Будет сгенерирован автоматически, если не определен". Я так и не понял, зачем это.

Exit Nodes - гипервизоры, которые должны быть настроены как шлюзы выхода из сети EVPN через реальную сеть. Настроенные узлы объявят маршрут по умолчанию в сети EVPN.

Primary Exit Node - если вы используете несколько выходных гипервизоров, вы можете перенаправляйть трафик через выбранный основной выходной узел вместо балансировки нагрузки на всех узлах. Необязательно, но необходимо, если вы хотите использовать SNAT или если ваш вышестоящий маршрутизатор не поддерживает ECMP

Advertise Subnets - анонс коннектед сеток в EVPN (я так понимаю, это анонс машрутов 5го типа). Неоходимо, если у вас "Тихие" виртуальные машины/ контейнеры (например, если у вас несколько IP-адресов и шлюз anycast не видит трафик с этих IP-адресов, IP-адреса не смогут быть доступны внутри сети EVPN)

Disable ARP ND Suppression - необходимо, если вы используете плавающие IP-адреса в своих виртуальных машинах (IP-и MAC-адреса перемещаются между системами).

Route-target Import - импорт, rt. Можно написать несколько через пробел

Опции далее, думаю, разбирать не стоит

После чего идем в Datacenter-SDN-Vnets-Create

![image](https://github.com/user-attachments/assets/53324136-805a-4fb5-a09b-86e885ef6720)

Где нужно выбрать имя Vnet, созданную ранее Zone и Tag (идентификатор VlanIf)

Далее в окошке справа "Suntets" создаем подсети для наших виртуалок (там же к ним можно и прикрутить dhcp сервер) 

![image](https://github.com/user-attachments/assets/a7c8f4a0-2b81-4e83-8712-349650f2a99b)

Стоит обратить внимание, что указанный шлюз станент Anycast gateway для наших вируальных машин. 

Далее в Datacenter-SDN. Жмем Apply - это дейсвие создаст виртульный адаптер, который мы будем прикреплять к виртуалкам и контейнерам. 

Откроем файл /etc/frr/frr.conf чтоб посмотреть, что мы там наконфигурировали в вэб-интерфейсе:

![image](https://github.com/user-attachments/assets/f03eb12d-c09c-44b6-b937-d7c18a960864)

![image](https://github.com/user-attachments/assets/df0718a6-c13e-4cda-a2c3-5ed234cd6fca)

И /etc/network/interfaces.d/sdn

![image](https://github.com/user-attachments/assets/77ca8c37-c45e-4a2f-9949-b05983c24044)



Теперь настало время настраивать underlay и с ним я напоролся на первую проблему- дело в том, что не смотря на заявлянную поддержку isis и bgp использовать их из графического интерфейса мы не можем, т.к. согласно оф.документации proxmox:

![image](https://github.com/user-attachments/assets/f0966a5c-9248-483a-b52a-fc486b49200c)

Он существует только для экспорта маршрутов EVPN в домен ISIS.
А что BGP?

![image](https://github.com/user-attachments/assets/bb10c844-4a5a-4987-89bc-21fb3c0b13f9)

...он также может быть использован для экспорта маршрутов EVPN во внешний одноранговый узел BGP - что также не подходит для пострение undelay сети. 

Как я писал ранее - мы можем использовать статические маршруты.

На proxmox 1:

![image](https://github.com/user-attachments/assets/b1e72eb3-33a6-4fd0-a49c-69f2ac3b6ece)

На proxmox 2:

![image](https://github.com/user-attachments/assets/9f9720ae-7a6c-471b-8781-26672da69525)

На обоих рестаруем службу сети:

systemctl restart networking

Отредактируем файл /etc/frr/daemons:

bgpd=yes\
bfdd=yes

Остальное без изменений, сохраняем, выходим. Рестаруем демона frr:

systemctl restart frr

Далее по схеме следует настройка роутера R5. В ней нет ничего особенного, потому приведу его running config:

![image](https://github.com/user-attachments/assets/7ff51572-97f1-4595-9c5e-7b57d8931ebc)

![image](https://github.com/user-attachments/assets/cb6474d5-e678-4410-97ce-4bb4d3f91514)

![image](https://github.com/user-attachments/assets/0217d273-c84c-4d3e-bfde-89b808838875)



Далее настроим proxmox 192.168.5.24 аналогично 192.168.5.23, разница только в ip адресах и тем, что на 192.168.5.24 мы создаем Subnets 172.17.0.0/24 gw - 172.17.0.1 и еще на всех гипервизорах нужно прописать одинаковый мак адрес VnetMac Address в меню SDN-Zones. Сам процесс настройки я описывать не буду


Вот настройки контейнера 1 (нужно изменить сетевую карту из стандартного бриджа vmbr0 на новосозданную):

![image](https://github.com/user-attachments/assets/3734fe70-21c2-48da-8e10-02d3ab8c60c7)

Вот настройки контейнера 2:

![image](https://github.com/user-attachments/assets/ad557de4-360c-4ca2-a008-1f5d315b0b26)

Проверим ip связанность (для начала нужно отпинговать шлюзы, чтоб маршут 2го типа "растекся" по evpn домену, а можно поставить галочку в "Datacenter-SDN-Zones" называющуюся "Advertise Subnets"):

![image](https://github.com/user-attachments/assets/338754d8-2425-4314-ada8-82baaf8c9335)


Рассмортим show команды для vtysh:

![image](https://github.com/user-attachments/assets/62173390-7a8a-4cad-b934-cab6c9ae89e4)

![image](https://github.com/user-attachments/assets/33de697f-a9b9-4ec2-be87-7c52fe00df8c)

![image](https://github.com/user-attachments/assets/c0f64f3f-fc42-4066-9ca6-de27dfcd296b)

![image](https://github.com/user-attachments/assets/e02db2f0-40b3-4b36-b2dd-ec55ba7147e8)



Вот дамп, видно, что все ok:

![image](https://github.com/user-attachments/assets/d08a6df0-b804-4a1b-876e-e8df3c4aabb2)

В заключении этой лабы попробуем зашифровать трафик. Как говорил эстонцец в анекдоте с дохлой вороной: "Пригодиццца", ему не пригодилось, а нам реально таки пригодится:

Изменим mtu на 1390:


![image](https://github.com/user-attachments/assets/097be5a2-5117-4c75-8615-82812f23a2be)


apt install strongswan

Открываем /etc/ipsec.conf

Дописываем:

conn %default
    ike=aes256-sha1-modp1024!  # the fastest, but reasonably secure cipher on modern HW
    esp=aes256-sha1!
    leftfirewall=yes           # this is necessary when using Proxmox VE firewall rules

conn output
    rightsubnet=%dynamic[udp/4789]
    right=%any
    type=transport
    authby=psk
    auto=route

conn input
    leftsubnet=%dynamic[udp/4789]
    type=transport
    authby=psk
    auto=route

  Генерируем ключ:

  openssl rand -base64 128

  И добавляем сгенерированный ключ в /etc/ipsec.secrets. Вид должен быть такой:

  PSK "сгенерированный ключ"


  ipsec update - запускаем

  ipsec statusall - проверяем статус

Запускаем пинги и смотрим. И вот:

![image](https://github.com/user-attachments/assets/4d2edcb2-6947-4d85-8b74-ff3fa5761dca)


![image](https://github.com/user-attachments/assets/211b5354-80d4-43f6-88b7-d6dbc63f00d0)

Могу от себя добавить, что с шифрованием на моем виртуальном стенде есть проблемы - работает в зависимости от того, как сойдутся звезды на небе. Потому в дадьнейшем я ipsec выключу на всех гипервизорах:

ipsec stop 


Ps: у меня все таки получилось настроить динамическую маршрутизацию и vxlan связанность между контейнерами, я потратил очень много времени искав проблему не в том месте - оказывается не сошелся ospf между первым проксмоксом и роутером cisco. Причем заработал он после того, как я перенес анонс сетей из секции ospf в меню конфигурирования интерфейсов. Я постоянно борюсь с какими-то багами виртуалзизации, может стоило брать intel, а сокет AM5 еще сыроват? 

Но что мы получим в итоге: гипервизор который настраивается в трех местах: файлы /etc/frr/frr.conf и /etc/network/interfaces.d/sdn, а также в графике самого проксмокса. Более того файлы frr.conf и sdn перезаписываются после обновления настроек в графике, более того нельзя просто взять и переписать файл sdn и рестартануть службу networking.  Чтоб виртуалки заработали с новыми настройками сети, их нужно также перезагрузить. Без головной боли нам не получить таких ништяков динамической маршрутизиции как ECMP и L3 отказоустойчивость,
также нет смылса стоить clos топологию.
Как по мне все это ставит крест на использовании vxlan в проксмокс для серьезных промышленных решений, что сами австрийцы и не скрывают - они пишут, что функционал SDN сейчас на стадии тестирования

  
  

  



























